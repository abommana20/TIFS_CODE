# -*- coding: utf-8 -*-
"""Resnet18 (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11FbsAeAhQ3S6parqemvpKzN4Y8mRpWV5
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet18, vgg16
from torch.utils.data import DataLoader
import numpy as np
import random
import math

import torch.nn.functional as F
random.seed(0)
np.random.seed(0)
torch.manual_seed(0)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(0)

class AverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

##################################Start of Functions######################
# Train and Test Functions
def train(model, train_loader, criterion, optimizer, epoch, device, print_interval=100):
    losses = AverageMeter()
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    for batch_idx, (data, targets) in enumerate(train_loader, 0):
        data, targets = data.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, targets)
        losses.update(loss.item(), data.size(0))
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += targets.size(0)
        correct += (predicted == targets).sum().item()

        # Print average loss and accuracy after completing the batch
        if (batch_idx) % print_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), train_loader.sampler.__len__(),
                        100. * batch_idx / len(train_loader), loss.item()))
    total_avg_loss = running_loss / len(train_loader)
    train_accuracy = 100 * correct / total
    print('\nTrain set: Avg Loss: {} Accuracy: {}/{} ({:.4f}%)\n'.format(losses.avg,
        correct, train_loader.sampler.__len__(),
        100. * correct / train_loader.sampler.__len__()))
    # print(f'Epoch {epoch}:  Avg Loss: {total_avg_loss}, Train Accuracy: {train_accuracy}%')


def test(model, test_loader, criterion, epoch, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            loss = criterion(outputs, target)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    test_accuracy = 100 * correct / total
    avg_test_loss = running_loss / len(test_loader)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\n'.format(
            avg_test_loss, correct, test_loader.sampler.__len__(),
            100. * correct / test_loader.sampler.__len__()))
    # print(f' Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}%')
    return avg_test_loss

def group_product(xs, ys):
    """
    the inner product of two lists of variables xs,ys
    :param xs:
    :param ys:
    :return:
    """
    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])

def get_trace_hut(model, loss, n_v,device, layerwise = False, filterwise=True):
    """
    Compute the trace of hessian using Hutchinson's method
    This approach requires computing only the application of the Hessian to a random input vector
    This has the same cost as backpropagating the gradient

    Rademacher vector v is a list of tensors that follows size of parameter tensors.
    Hessian vector product Hv is a tuple of tensors that follows size of parameter tensors.
    Final result trace_vHv is a 3D tensor containing 300 x vHv for each channel in each layer.

    """

    params = []
    conv_fc = []
    for name, param in model.named_parameters():
        if 'bias' not in name and param.requires_grad:
            if 'conv' in name or ('downsample.0' in name):
                # print(name)
                params.append(param)
                conv_fc.append(0)
            if 'fc' in name:
                params.append(param)
                conv_fc.append(1)

    gradsH = torch.autograd.grad(loss, params, create_graph=True)
    # print(conv_fc)
    trace_vhv = []

    for i in range(n_v):
            # Sampling a random vector from the Rademacher Distribution
            # print(i)
            v = [torch.randint_like(p, high=2, device=device).float() * 2 - 1 for p in params]
            # print(v.shape)
            # Calculate 2nd order gradients in FP32
            # stop_criterion=(i == (n_v - 1))
            Hv = torch.autograd.grad(gradsH, params, grad_outputs=v,only_inputs=True, retain_graph=True)
            # print(len(Hv))
            # for hv_i in Hv:
                # print(hv_i.shape)

            # v = [vi.detach().cpu() for vi in v]
            # Hv = [Hvi.detach().cpu() for Hvi in Hv]

            with torch.no_grad():
                if filterwise:
                    trace_iter = []
                    for layer_i in range(len(Hv)):
                        trace_layer = []
                        if conv_fc[layer_i] == 0 :
                            for out_channel_i in range(Hv[layer_i].size(0)):
                                for in_channel_i in range(Hv[layer_i][out_channel_i].size(0)):
                                    trace_layer.append(Hv[layer_i][out_channel_i][in_channel_i].flatten().dot(v[layer_i][out_channel_i][in_channel_i].flatten()).item())
                        else:
                            for out_channel_i in range(Hv[layer_i].size(0)):
                                    # print(Hv[layer_i].size(0))
                                    trace_layer.append(Hv[layer_i][out_channel_i].flatten().dot(v[layer_i][out_channel_i].flatten()).item())
                        trace_iter.append(trace_layer)
                    # print(len(trace_iter))
                    trace_vhv.append(trace_iter)
                elif layerwise:
                    trace_layer = []
                    for Hv_i in range(len(Hv)):
                        trace_layer.append(Hv[Hv_i].flatten().dot(v[Hv_i].flatten()).item())
                    trace_vhv.append(trace_layer)
                else:
                    trace_vhv.append(group_product(Hv, v).item())

    ##DO Average
    # print(len(trace_vhv))
    avg_trace =[]
    if filterwise:
        # Iterate through each pair of subsublists
        for subsublists in zip(*[sublist for sublist in trace_vhv]):
            # For each pair, zip their elements, sum them, and divide by the length of A
            averaged_subsublist = [sum(elements) / len(trace_vhv) for elements in zip(*subsublists)]
            avg_trace.append(averaged_subsublist)
        # print(averaged)

    elif layerwise:
        average_value = [sum(elements) / len(trace_vhv) for elements in zip(*trace_vhv)]
        avg_trace = (average_value)
    else:
        avg_trace.append(sum(trace_vhv) / len(trace_vhv))

    return avg_trace

def num_pad( source, target):
    crxb_index = math.ceil(source / target)
    num_padding = crxb_index * target - source
    return crxb_index, num_padding

def num_pad_col(source, target):
    #crxb_col_fit = target/(self.quantize_weights/2)
    crxb_index = math.ceil((source * (H//B)) / target)
    num_padding =int( crxb_index * target / (H//B) - source)
    return crxb_index, num_padding

def find_M(k, loss, ntrace, H, B):
    D = H // B
    max_loss = (100 + k) * loss / 100
    print("max loss allowable", max_loss)
    M_opt_k = []  # List to store low_a and low_d for each j in this iteration of k

    for j in range(0, len(ntrace)):  # Iterating over j
        low_d = 0
        for i in range(1, D + 1):
            rhs = (2 * (max_loss - loss) * (2**H - 1)**2) / (2**(i * B) - 1)**2
            if ntrace[j] < rhs:
                if i > low_d:
                    low_d = i
        # Store the results for each j in low_k
        M_opt_k.append({
            "layer": j,
            "M_opt_diag": low_d
        })
    return M_opt_k
def find_M_network(k, loss, ntrace, H, B):
    D = H // B
    max_loss = (100 + k) * loss / 100
    # print("max loss allowable", max_loss)
    # List to store low_a and low_d for each j in this iteration of k

    for j in range(0, len(ntrace)):  # Iterating over j
        low_d = 0
        for i in range(1, D + 1):
            rhs = (2 * (max_loss - loss) * (2**H - 1)**2) / (2**(i * B) - 1)**2
            if ntrace[j] < rhs:
                if i > low_d:
                    low_d = i
        # Store the results for each j in low_k

    return low_d

def get_top_indices(model, ftrace, H, B,crossbar_size=128):
    col_size = int(crossbar_size/(H//B))
    params = []
    conv_fc = []
    for name, param in model.named_parameters():
        if 'bias' not in name and param.requires_grad:
            if 'conv' in name or ('downsample.0' in name):
                # print(name)
                params.append(param)
                conv_fc.append(0)
            if 'fc' in name:
                params.append(param)
                conv_fc.append(1)


    indices_per_layer = []
    for idx,layer in enumerate(params):
        hessian_matrix = []
        if conv_fc[idx] == 0:
            size = layer.shape
            print(len(ftrace[idx]))
            for i in range(len(ftrace[idx])):
                hessian_matrix.append(torch.full((size[2], size[3]), ftrace[idx][i]))
            # No of in Channels
            ic = size[1]
            P = [torch.stack(hessian_matrix[i:i+ic]) for i in range(0, len(hessian_matrix), ic)]
            FP = torch.stack(P,dim=0)
            # print(FP.size())
            FP_flatten = FP.view(FP.shape[0], -1)
            #print("weights_flatten size: ", weight_flatten.size())
            #print("weights_ size: ", self.weight.size())
            crxb_row, crxb_row_pads = num_pad(FP_flatten.shape[1], crossbar_size)
            crxb_col, crxb_col_pads = num_pad_col(FP_flatten.shape[0], crossbar_size)
            w_pad = (0, crxb_row_pads, 0, crxb_col_pads)
            weight_padded = F.pad(FP_flatten, w_pad,mode='constant', value=0)
            weight_crxb = weight_padded.view(crxb_col, col_size, crxb_row, crossbar_size).transpose(1, 2)
            # print(weight_crxb.size())
            crxb_shape = weight_crxb.size()
            # Flatten each c x d sub-tensor and concatenate
            flattened = weight_crxb.reshape(crxb_shape[0], crxb_shape[1], -1)

            # Sort the values along the last dimension in descending order
            sorted_values, sorted_indices = flattened.sort(dim=-1, descending=True)
            indices_per_layer.append(sorted_indices)
        else:
            size = layer.shape
            print(len(ftrace[idx]))
            for i in range(len(ftrace[idx])):
                T = torch.empty(size[0],size[1])
                T[i] = ftrace[idx][i]
                hessian_matrix.append(T)
            # No of in Channels
            ic = size[1]
            FP = hessian_matrix[0]
            # print(FP.size())
            FP_flatten = FP.view(FP.shape[0], -1)
            #print("weights_flatten size: ", weight_flatten.size())
            #print("weights_ size: ", self.weight.size())
            crxb_row, crxb_row_pads = num_pad(FP_flatten.shape[1], crossbar_size)
            crxb_col, crxb_col_pads = num_pad_col(FP_flatten.shape[0], crossbar_size)
            w_pad = (0, crxb_row_pads, 0, crxb_col_pads)
            weight_padded = F.pad(FP_flatten, w_pad,mode='constant', value=0)
            weight_crxb = weight_padded.view(crxb_col, col_size, crxb_row, crossbar_size).transpose(1, 2)
            # print(weight_crxb.size())
            crxb_shape = weight_crxb.size()
            # Flatten each c x d sub-tensor and concatenate
            flattened = weight_crxb.reshape(crxb_shape[0], crxb_shape[1], -1)

            # Sort the values along the last dimension in descending order
            sorted_values, sorted_indices = flattened.sort(dim=-1, descending=True)
            indices_per_layer.append(sorted_indices)
    return indices_per_layer
def inject_fault(qmodel, indices_tensor, fault_rate, M, H, B,crossbar_size=128):
    col_size = int(crossbar_size/(H//B))
    qmodel.load_state_dict(torch.load('quantized_resnet_model.pth'))
    delta_w =  max_weight * (2**(M*B)-1)/(2**H-1)
    i = 0
    for name, param in (qmodel.named_parameters()):
        if 'bias' not in name and param.requires_grad:
            if 'conv' in name or 'fc' in name or ('downsample.0' in name):
                # print(i)
                P_flatten = param.data.view(param.shape[0], -1)
                # print(P_flatten.shape)
                crxb_row, crxb_row_pads = num_pad(P_flatten.shape[1], crossbar_size)
                crxb_col, crxb_col_pads = num_pad_col(P_flatten.shape[0], crossbar_size)
                w_pad = (0, crxb_row_pads, 0, crxb_col_pads)
                # print(crxb_row_pads,crxb_col_pads)
                weight_padded = F.pad(P_flatten, w_pad,mode='constant', value=0).to(device)
                # print(weight_padded.shape)
                weight_crxb = weight_padded.view(crxb_col, col_size, crxb_row, crossbar_size).transpose(1, 2)
                # print(weight_crxb.shape)
                crxb_shape = weight_crxb.shape
                flattened = weight_crxb.reshape(crxb_shape[0], crxb_shape[1], -1).to(device)
                # print(flattened.shape)
                # For each crossbar array
                for crxb_r in range(crxb_shape[0]):
                    for crxb_c in range(crxb_shape[1]):
                        # Calculate number of elements to adjust based on fault rate
                        num_elements_to_adjust = int(flattened.shape[-1] * fault_rate / 100)

                        # Get indices for the top elements to adjust
                        # Calculate the number of elements to adjust from each half
                        elements_from_each_half = num_elements_to_adjust // 2

                        # Select elements from the start
                        # first_half_indices = indices_tensor[i][crxb_r, crxb_c, :elements_from_each_half].to(device)

                        # # Select elements from the end
                        # second_half_indices = indices_tensor[i][crxb_r, crxb_c, -elements_from_each_half:].to(device)

                        # # Combine the indices from both halves
                        # top_indices = torch.cat((first_half_indices, second_half_indices))

                        top_indices = indices_tensor[i][crxb_r, crxb_c, :num_elements_to_adjust].to(device)
                        # top_indices = indices_tensor[i][crxb_r, crxb_c, -num_elements_to_adjust:].to(device)

                        # Get indices for the top elements to adjust
                        top_indices = indices_tensor[i][crxb_r, crxb_c, :num_elements_to_adjust].to(device)
                        # top_indices = indices_tensor[i][crxb_r, crxb_c, -num_elements_to_adjust:].to(device)
                        # Select elements to be adjusted
                        selected_elements = torch.index_select(flattened[crxb_r, crxb_c], 0, top_indices).to(device)

                        # Apply delta_w to these elements
                        adjusted_elements = selected_elements + delta_w

                        # Place adjusted elements back into the flattened tensor
                        flattened[crxb_r, crxb_c].scatter_(0, top_indices, adjusted_elements).to(device)

                    # Reverse the reshaping operation
                modified_weight_crxb = flattened.view(crxb_shape[0], crxb_shape[1], col_size, crossbar_size)
                # print(modified_weight_crxb.shape)
                modified_weight_crxb = modified_weight_crxb.transpose(1, 2)
                modified_weight_padded = modified_weight_crxb.reshape(weight_padded.shape[0], -1)
                # print(modified_weight_padded.shape)
                if crxb_row_pads > 0:
                    weight_unpadded = modified_weight_padded[:, 0:modified_weight_padded.shape[1]-crxb_row_pads]
                else:
                    weight_unpadded = modified_weight_padded

                # Remove row padding (if crxb_row_pads > 0)
                # print(crxb_row_pads)
                # print(crxb_col_pads)
                if crxb_col_pads > 0:
                    weight_unpadded = modified_weight_padded[0:weight_padded.shape[0]-crxb_col_pads, :]

                # print(weight_unpadded.shape)
                modified_P_flatten = weight_unpadded.view(P_flatten.shape)

                # Reshape back to the original param shape
                param.data = weight_unpadded.view(param.shape)

                # print(flattened.shape)
                # print(i)
                # print(indices_tensor[i].shape)
                i = i+1


def quantize(max_weight, num_bits=16):
    qmax = 2**(num_bits - 1) - 1
    # max_val = tensor.abs().max()
    scale = max_weight / qmax
    # quantized = torch.round(tensor / scale)
    return scale

def dequantize(quantized_tensor, scale):
    return quantized_tensor * scale
##################################End of Functions######################

''''Code start here'''''



"""Define Variables"""

H=16
B=2
crossbar_size = 128
batch_sizes = [1,16,32,64,128,256,512,1024]
fault_rates = [1]


# Data Preparation
normalize = transforms.Normalize(
        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
    )
kwargs = {'num_workers': 2, 'pin_memory': True}
train_dataset = torchvision.datasets.CIFAR10('./data/cifar-10', train=True, download=True,
                                     transform=transforms.Compose([transforms.RandomHorizontalFlip(),
                                      transforms.RandomCrop(32, 4),
                                      transforms.ToTensor(), normalize]))
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(torchvision.datasets.CIFAR10('./data/cifar-10', train=False,
        transform=transforms.Compose([transforms.ToTensor(), normalize])),
        batch_size=64, shuffle=True, **kwargs)
# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
# trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)
# testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
# testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)

# Check if a GPU is available and use it; otherwise, use the CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Model Selection
model = resnet18(weights=None).to(device) # or vgg16(pretrained=False)
model.fc = nn.Linear(model.fc.in_features, 10).to(device)  # Adjusting for CIFAR10 output

# Loss Function and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

print(model)

'''start tarining'''
best_loss = float('inf')
epochs_no_improve = 0
patience = 10  # Number of epochs to wait for improvement before stopping

for epoch in range(50):  # Set a maximum epoch
    train(model, train_loader, criterion, optimizer, epoch, device)
    test_loss = test(model, test_loader, criterion, epoch, device)

    # Reduce learning rate based on the test loss
    scheduler.step(test_loss)

    # Save the model if the test loss improved
    if test_loss < best_loss:
        best_loss = test_loss
        torch.save(model.state_dict(), 'best_model.pth')
        epochs_no_improve = 0
        print(f"Epoch {epoch}: Test loss improved, saving model.")
    else:
        epochs_no_improve += 1
        print(f"Epoch {epoch}: No improvement in test loss for {epochs_no_improve} epochs.")

    # Early stopping
    if epochs_no_improve >= patience:
        print("Early stopping triggered due to no improvement in test loss.")
        break

"""Load the model and do inference."""

# Load the model state dict from the .pth file
lmodel = resnet18(weights=None).to(device) # or vgg16(pretrained=False)
lmodel.fc = nn.Linear(model.fc.in_features, 10).to(device)  # Adjusting for CIFAR10 output
lmodel.load_state_dict(torch.load('./best_model.pth'))

lmodel.to(device)
# print(model)

# Find the maximum weight in the network

max_weight = float('-inf')  # Initialize to negative infinity

for name, param in model.named_parameters():
    if 'bias' not in name and param.requires_grad:
        if 'conv' in name or 'fc' in name or ('downsample.0' in name):
            max_weight = max(max_weight, param.data.max().item())

print(f'The maximum weight in the network is: {max_weight}')

# Apply quantization and dequantization to the model's parameters. If you want to save the quantized weights
quantized_state_dict = {}
scale = quantize(max_weight, num_bits=H)
for name, param in model.named_parameters():
    if 'bias' not in name and param.requires_grad:
        if 'conv' in name or 'fc' in name or ('downsample.0' in name):
            quantized_tensor= torch.round(param.data/scale)
            dequantized_tensor = dequantize(quantized_tensor, scale)
            quantized_state_dict[name] = dequantized_tensor
        else:
            quantized_state_dict[name] = param.data
    else:
        quantized_state_dict[name] = param.data

for name, param in model.state_dict().items():
    if 'running_mean' in name or 'running_var' in name:
        # Handle running mean and variance
        quantized_state_dict[name] = param
# Save the quantized model
torch.save(quantized_state_dict, 'quantized_resnet_model.pth')
print('Saved quantized model to quantized_resnet_model.pth')

#finding the right indces as per crossbar structure
params = []
conv_fc = []
for name, param in model.named_parameters():
    if 'bias' not in name and param.requires_grad:
        if 'conv' in name or ('downsample.0' in name):
            # print(name)
            params.append(param)
            conv_fc.append(0)
        if 'fc' in name:
            params.append(param)
            conv_fc.append(1)
# indices_per_layer = get_top_indices(model, ftrace, H, B,crossbar_size=128)
# print(len(indices_per_layer))
# torch.save(indices_per_layer,'indices.pt')

qmodel = resnet18(weights=None).to(device) #
qmodel.fc = nn.Linear(model.fc.in_features, 10).to(device)  # Adjusting for CIFAR10 output
qmodel.load_state_dict(torch.load('quantized_resnet_model.pth'))

scale = quantize(max_weight, num_bits=H)
with torch.no_grad():
    for name, param in model.named_parameters():
        # print(name)
        if 'bias' not in name and param.requires_grad:
            if 'conv' in name or 'fc' in name or ('downsample.0' in name):
                param.copy_(torch.round(param / scale) * scale)
# quantized_model.eval()
model.eval()

M_b = {}
loss_b = {}
crct_pred_b = {}
hessian_b = {}
for b in batch_sizes:
    test_loader_b = torch.utils.data.DataLoader(torchvision.datasets.CIFAR10('./data/cifar-10', train=False,
        transform=transforms.Compose([transforms.ToTensor(), normalize])),
        batch_size=b, shuffle=False, **kwargs)
    model.eval()
    M_b[f'batch_size_{b}'] = []
    M = M_b[f'batch_size_{b}']
    loss_b[f'batch_size_{b}'] = []
    loss_avg = loss_b[f'batch_size_{b}']
    crct_pred_b[f'batch_size_{b}'] = []
    crct_pred = crct_pred_b[f'batch_size_{b}']
    hessian_b[f'batch_size_{b}'] = []
    hessian = hessian_b[f'batch_size_{b}']
    for batch_idx, (data, targets) in enumerate(test_loader_b, 0):
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            loss = criterion(outputs, targets)
            _, predicted = torch.max(outputs.data, 1)
            total = targets.size(0)
            correct = (predicted == targets).sum().item()
            ntrace = get_trace_hut(model, loss,100, device, layerwise = False, filterwise=False)
            hessian.append(ntrace[0])
            M_opt=find_M_network(15, loss, ntrace, H, B)
            M.append(M_opt)
            loss_avg.append(loss.item())
            crct_pred.append(correct)
            # if batch_idx == 5:
                # break
            # print(M)
torch.save(M_b,'optimal_m_batch.pt')
torch.save(loss_b,'avgloss_batch.pt')
torch.save(crct_pred_b,'crct_pred_batch.pt')
torch.save(hessian_b,'hessian_batch.pt')

